{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pommerman Demo.\n",
    "\n",
    "This notebook demonstrates how to train Pommerman agents. Please let us know at support@pommerman.com if you run into any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random agents\n",
    "\n",
    "The following codes instantiates the environment with four random agents who take actions until the game is finished. (This will be a quick game.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add four random agents\n",
    "agents = {}\n",
    "for agent_id in range(4):\n",
    "    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "env.set_agents(list(agents.values()))\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': <Result.Win: 0>, 'winners': [3]}\n"
     ]
    }
   ],
   "source": [
    "# Seed and reset the environment\n",
    "env.seed(0)\n",
    "obs = env.reset()\n",
    "\n",
    "# Run the random agents until we're done\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    actions = env.act(obs)\n",
    "    obs, reward, done, info = env.step(actions)\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Agents\n",
    "\n",
    "The following code runs the environment with 3 random agents and one agent with human input (use the arrow keys on your keyboard). This can also be called on the command line with:\n",
    "\n",
    "`python run_battle.py --agents=player::arrows,random::null,random::null,random::null --config=PommeFFACompetition-v0`\n",
    "\n",
    "You can also run this with SimpleAgents by executing:\n",
    "\n",
    "`python run_battle.py --agents=player::arrows,test::agents.SimpleAgent,test::agents.SimpleAgent,test::agents.SimpleAgent --config=PommeFFACompetition-v0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "\n",
    "# Add 3 random agents\n",
    "agents = {}\n",
    "for agent_id in range(3):\n",
    "    agents[agent_id] = RandomAgent(config[\"agent\"](agent_id, config[\"game_type\"]))\n",
    "\n",
    "# Add human agent\n",
    "agents[3] = PlayerAgent(config[\"agent\"](agent_id, config[\"game_type\"]), \"arrows\")\n",
    "\n",
    "env.set_agents(list(agents.values()))\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': <Result.Win: 0>, 'winners': [3]}\n"
     ]
    }
   ],
   "source": [
    "# Seed and reset the environment\n",
    "env.seed(0)\n",
    "obs = env.reset()\n",
    "\n",
    "# Run the agents until we're done\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    actions = env.act(obs)\n",
    "    obs, reward, done, info = env.step(actions)\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "\n",
    "# Print the result\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Agent\n",
    "\n",
    "The following code uses Tensorforce to train a PPO agent. This is in the train_with_tensorforce.py module as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TensorForceError' from 'tensorforce' (c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorforce\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-a7b6be10fb12>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorforce\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0magents\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPPOAgent\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorforce\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecution\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRunner\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorforce\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontrib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopenai_gym\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mOpenAIGym\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorforce\\contrib\\openai_gym.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mgym\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrappers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorforce\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTensorForceError\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorforce\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menvironments\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mEnvironment\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'TensorForceError' from 'tensorforce' (c:\\users\\marku\\anaconda3\\envs\\pommerman\\lib\\site-packages\\tensorforce\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Make sure you have tensorforce installed: pip install tensorforce\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_np_float(feature):\n",
    "    return np.array(feature).astype(np.float32)\n",
    "\n",
    "def featurize(obs):\n",
    "    board = obs[\"board\"].reshape(-1).astype(np.float32)\n",
    "    bomb_blast_strength = obs[\"bomb_blast_strength\"].reshape(-1).astype(np.float32)\n",
    "    bomb_life = obs[\"bomb_life\"].reshape(-1).astype(np.float32)\n",
    "    position = make_np_float(obs[\"position\"])\n",
    "    ammo = make_np_float([obs[\"ammo\"]])\n",
    "    blast_strength = make_np_float([obs[\"blast_strength\"]])\n",
    "    can_kick = make_np_float([obs[\"can_kick\"]])\n",
    "\n",
    "    teammate = obs[\"teammate\"]\n",
    "    if teammate is not None:\n",
    "        teammate = teammate.value\n",
    "    else:\n",
    "        teammate = -1\n",
    "    teammate = make_np_float([teammate])\n",
    "\n",
    "    enemies = obs[\"enemies\"]\n",
    "    enemies = [e.value for e in enemies]\n",
    "    if len(enemies) < 3:\n",
    "        enemies = enemies + [-1]*(3 - len(enemies))\n",
    "    enemies = make_np_float(enemies)\n",
    "\n",
    "    return np.concatenate((board, bomb_blast_strength, bomb_life, position, ammo, blast_strength, can_kick, teammate, enemies))\n",
    "\n",
    "class TensorforceAgent(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'batching_capacity'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-3ff3b2cb0b28>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m     step_optimizer=dict(\n\u001B[0;32m     16\u001B[0m         \u001B[0mtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'adam'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m         \u001B[0mlearning_rate\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1e-4\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m     )\n\u001B[0;32m     19\u001B[0m )\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'batching_capacity'"
     ]
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_fast_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "env.seed(0)\n",
    "\n",
    "# Create a Proximal Policy Optimization agent\n",
    "agent = PPOAgent(\n",
    "    states=dict(type='float', shape=env.observation_space.shape),\n",
    "    actions=dict(type='int', num_actions=env.action_space.n),\n",
    "    network=[\n",
    "        dict(type='dense', size=64),\n",
    "        dict(type='dense', size=64)\n",
    "    ],\n",
    "    batching_capacity=1000,\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-4\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add 3 random agents\n",
    "agents = []\n",
    "for agent_id in range(3):\n",
    "    agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "\n",
    "# Add TensorforceAgent\n",
    "agent_id += 1\n",
    "agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "env.set_agents(agents)\n",
    "env.set_training_agent(agents[-1].agent_id)\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAIGym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-3e6660b2cd85>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mclass\u001B[0m \u001B[0mWrappedEnv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mOpenAIGym\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgym\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvisualize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgym\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgym\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvisualize\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvisualize\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'OpenAIGym' is not defined"
     ]
    }
   ],
   "source": [
    "class WrappedEnv(OpenAIGym):    \n",
    "    def __init__(self, gym, visualize=False):\n",
    "        self.gym = gym\n",
    "        self.visualize = visualize\n",
    "    \n",
    "    def execute(self, action):\n",
    "        if self.visualize:\n",
    "            self.gym.render()\n",
    "\n",
    "        actions = self.unflatten_action(action=action)\n",
    "            \n",
    "        obs = self.gym.get_observations()\n",
    "        all_actions = self.gym.act(obs)\n",
    "        all_actions.insert(self.gym.training_agent, actions)\n",
    "        state, reward, terminal, _ = self.gym.step(all_actions)\n",
    "        agent_state = featurize(state[self.gym.training_agent])\n",
    "        agent_reward = reward[self.gym.training_agent]\n",
    "        return agent_state, terminal, agent_reward\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.gym.reset()\n",
    "        agent_obs = featurize(obs[3])\n",
    "        return agent_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats:  [-1, -1, -1, -1, -1] [15, 15, 27, 32, 26] [2.0443358421325684, 0.7581827640533447, 1.3421897888183594, 1.6136739253997803, 1.2573180198669434]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and run the environment for 5 episodes.\n",
    "wrapped_env = WrappedEnv(env, True)\n",
    "runner = Runner(agent=agent, environment=wrapped_env)\n",
    "runner.run(episodes=5, max_episode_timesteps=2000)\n",
    "print(\"Stats: \", runner.episode_rewards, runner.episode_timesteps, runner.episode_times)\n",
    "\n",
    "try:\n",
    "    runner.close()\n",
    "except AttributeError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}